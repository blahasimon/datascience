{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Method 3: Neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import kaleido\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import numdifftools as nd\n",
    "from numpy.random import uniform\n",
    "import math as m\n",
    "import time\n",
    "from numba import jit\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split , KFold\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "# setup offline mode\n",
    "py.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.09003057, 0.24472847, 0.66524096])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CZ_ALPHABET = ['a', 'á', 'b', 'c', 'č', 'd', 'ď',\n",
    "               'e', 'é', 'ě', 'f', 'g', 'h', 'ch',\n",
    "               'i', 'í', 'j', 'k', 'l', 'm', 'n',\n",
    "               'ň', 'o', 'ó', 'p', 'q', 'r', 'ř',\n",
    "               's', 'š', 't', 'ť', 'u', 'ú','ů',\n",
    "               'v', 'w', 'x', 'y', 'ý','z', 'ž', ' ']\n",
    "\n",
    "def word_to_vector(word: str) -> np.array:\n",
    "    array = np.zeros(len(CZ_ALPHABET))\n",
    "    for i, alph in enumerate(CZ_ALPHABET):\n",
    "        array[i] = word.count(alph)\n",
    "    return pd.Series(array)\n",
    "\n",
    "def calculate_hidden_layer_size(Ns: int, Ni: int, No: int, alpha: int) -> float:\n",
    "    Nh = Ns / (alpha * (Ni - No))\n",
    "    return Nh\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1 / (1 + np.e ** (-x))\n",
    "    return y\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    y = sigmoid(x) * (1 - sigmoid(x))\n",
    "    return y\n",
    "\n",
    "def ReLU(x):\n",
    "    y = x if x >= 0 else 0\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       Obec                  Kraj         a    á         b         c    č  \\\n0  abertamy      Karlovarský kraj  0.632456  0.0  0.316228  0.000000  0.0   \n1    adamov        Jihočeský kraj  0.707107  0.0  0.000000  0.000000  0.0   \n2    adamov     Jihomoravský kraj  0.707107  0.0  0.000000  0.000000  0.0   \n3    adamov      Středočeský kraj  0.707107  0.0  0.000000  0.000000  0.0   \n4  adršpach  Královéhradecký kraj  0.603023  0.0  0.000000  0.301511  0.0   \n\n          d    ď         e  ...    ů         v    w    x         y    ý    z  \\\n0  0.000000  0.0  0.316228  ...  0.0  0.000000  0.0  0.0  0.316228  0.0  0.0   \n1  0.353553  0.0  0.000000  ...  0.0  0.353553  0.0  0.0  0.000000  0.0  0.0   \n2  0.353553  0.0  0.000000  ...  0.0  0.353553  0.0  0.0  0.000000  0.0  0.0   \n3  0.353553  0.0  0.000000  ...  0.0  0.353553  0.0  0.0  0.000000  0.0  0.0   \n4  0.301511  0.0  0.000000  ...  0.0  0.000000  0.0  0.0  0.000000  0.0  0.0   \n\n     ž                                         expected  \n0  0.0  0.0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n1  0.0  0.0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n2  0.0  0.0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n3  0.0  0.0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n4  0.0  0.0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n\n[5 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Obec</th>\n      <th>Kraj</th>\n      <th>a</th>\n      <th>á</th>\n      <th>b</th>\n      <th>c</th>\n      <th>č</th>\n      <th>d</th>\n      <th>ď</th>\n      <th>e</th>\n      <th>...</th>\n      <th>ů</th>\n      <th>v</th>\n      <th>w</th>\n      <th>x</th>\n      <th>y</th>\n      <th>ý</th>\n      <th>z</th>\n      <th>ž</th>\n      <th></th>\n      <th>expected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abertamy</td>\n      <td>Karlovarský kraj</td>\n      <td>0.632456</td>\n      <td>0.0</td>\n      <td>0.316228</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.316228</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.316228</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>adamov</td>\n      <td>Jihočeský kraj</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.353553</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.353553</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>adamov</td>\n      <td>Jihomoravský kraj</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.353553</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.353553</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>adamov</td>\n      <td>Středočeský kraj</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.353553</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.353553</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>adršpach</td>\n      <td>Královéhradecký kraj</td>\n      <td>0.603023</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.301511</td>\n      <td>0.0</td>\n      <td>0.301511</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 46 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv', encoding = 'ansi', usecols=['Obec', 'Kraj'])\n",
    "data['Obec'] = data['Obec'].str.lower()\n",
    "output_dict = list(np.sort(data['Kraj'].unique()).flatten())\n",
    "\n",
    "data[CZ_ALPHABET] = data['Obec'].apply(word_to_vector)\n",
    "data['expected'] = data['Kraj'].apply(lambda x: (np.array(output_dict) == x).astype(int))\n",
    "\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the following to get the size of the hidden layer\n",
    "\n",
    "$$\n",
    "N_\\mathrm{h} = \\frac{N_\\mathrm{s}}{\\alpha (N_\\mathrm{i} + N_\\mathrm{o})}\n",
    "$$\n",
    "- $N_\\mathrm{s}$ - number of samples in training data\n",
    "- $N_\\mathrm{i}$ - number of input neurons (43)\n",
    "- $N_\\mathrm{o}$ - number of output neurons (14)\n",
    "- $\\alpha$ - an arbitrary number usually between 2 and 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, size: np.array, func: str = 'sigmoid', x_train: np.ndarray = None,\n",
    "                 y_train: np.array = None, eta: float = 0.01, backpropagation: bool = False):\n",
    "\n",
    "        self.train = x_train\n",
    "        self.expected = y_train\n",
    "        self.size = size\n",
    "        self.func = func\n",
    "        self.eta = eta\n",
    "        self.backpropagation = backpropagation\n",
    "        self.layers = np.array([uniform(size=n) for n in size], dtype=object)\n",
    "\n",
    "        self.n_layers = len(size)\n",
    "\n",
    "        self.n_weights = np.dot(size[:-1], size[1:])\n",
    "        self.n_biases = np.sum(size[1:])\n",
    "\n",
    "        self.n_params = self.n_weights + self.n_biases\n",
    "        self.params = uniform(size=self.n_params)\n",
    "        # initialize weights\n",
    "        self.W = None\n",
    "        self.update_weights()\n",
    "        # initialize biases\n",
    "        self.B = None\n",
    "        self.update_biases()\n",
    "\n",
    "    def a(self, L: int, k: int):\n",
    "        if L == 0:\n",
    "            a = layers[0, k]\n",
    "        else:\n",
    "            a = sigmoid(self.z(L=L, k=k))\n",
    "        return a\n",
    "\n",
    "    def z(self, L: int, k: int):\n",
    "        z = np.dot(self.W[L-1, k] * self.layers[L-1]) + self.B[L-1, k]\n",
    "        return z\n",
    "\n",
    "    def w(self, L: int, i: int, j: int):\n",
    "        w = self.W[L-1][i, j]\n",
    "        return w\n",
    "\n",
    "    def b(self, L: int, k: int):\n",
    "        b = self.B[L-1, k]\n",
    "        return b\n",
    "\n",
    "    def update_weights(self):\n",
    "        weights = self.params[:self.n_weights]\n",
    "        lengths = [self.size[i] * self.size[i+1] for i in range(self.n_layers-1)]\n",
    "        slices = self.slice_from_lengths(lengths=lengths)\n",
    "        weights = np.split(weights, slices)[:-1]\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            newshape = (self.size[i+1], self.size[i])\n",
    "            weights[i] = weights[i].reshape(newshape)\n",
    "        self.W = weights\n",
    "\n",
    "    def update_biases(self):\n",
    "        biases = self.params[:self.n_biases]\n",
    "        lengths = self.size[1:]\n",
    "        slices = self.slice_from_lengths(lengths=lengths)\n",
    "        biases = np.split(biases, slices)\n",
    "        self.B = biases\n",
    "\n",
    "    def propagate_forward(self):\n",
    "        self.update_weights()\n",
    "        self.update_biases()\n",
    "        f = ReLU if self.func == 'ReLU' else sigmoid\n",
    "        for i in range(self.n_layers-1):\n",
    "            self.layers[i+1] = f(np.dot(self.W[i], self.layers[i]) + self.B[i])\n",
    "\n",
    "    def feed_input(self, layer0):\n",
    "        self.layers[0] = layer0\n",
    "\n",
    "    def get_output(self):\n",
    "        return self.layers[-1]\n",
    "\n",
    "    def slice_from_lengths(self, lengths):\n",
    "        slices = [np.sum(lengths[:i]) for i in range(self.n_layers)][1:]\n",
    "        return slices\n",
    "\n",
    "    def predict(self, vector: np.array):\n",
    "        if vector.shape[0] != self.size[0]:\n",
    "            print(f'Vector shape {vector.shape} does not match layer shape {self.size[0]}')\n",
    "            raise TypeError\n",
    "        self.feed_input(vector)\n",
    "        self.propagate_forward()\n",
    "        prediction = self.get_output()\n",
    "        return prediction\n",
    "\n",
    "    def predict_word(self, word: str):\n",
    "        vector = word_to_vector(word)\n",
    "        return self.predict(vector)\n",
    "\n",
    "    def cost(self, vector: np.array, expected: np.array):\n",
    "        prediction = self.predict(vector)\n",
    "        c = np.sum((prediction - expected) ** 2)\n",
    "        return c\n",
    "\n",
    "    def cost_of_sample(self, params: np.ndarray):\n",
    "        sample_size = len(self.train)\n",
    "        self.params = params\n",
    "        costs = np.array([self.cost(self.train[i], self.expected[i]) for i in range(sample_size)])\n",
    "        return np.mean(costs)\n",
    "\n",
    "    def neg_grad(self):\n",
    "        if self.backpropagation:\n",
    "            grad = 0\n",
    "        else:\n",
    "            grad = nd.Gradient(self.cost_of_sample)(self.params)\n",
    "        return -grad\n",
    "\n",
    "    def d_l(self, l: int):\n",
    "        if l == self.n_layers - 1:\n",
    "            d_l = np.multiply(self.dE_dA(l), self.dA_dZ(l))\n",
    "        else:\n",
    "            d_l = np.dot(self.d_l(l+1).T, self.W[l])\n",
    "            d_l = np.multiply(d_l, self.dA_dZ(l))\n",
    "        return d_l\n",
    "\n",
    "    def X(self, l: int):\n",
    "        return self.layers[l]\n",
    "\n",
    "    def dE_dA(self, l: int):\n",
    "        if l == self.n_layers - 1:\n",
    "            pass\n",
    "\n",
    "    def dA_dZ(self, l: int):\n",
    "        pass\n",
    "\n",
    "    def dE_dW(self, l: int):\n",
    "        dE_dW = np.multiply(self.d_l(l), self.X(l-1))\n",
    "\n",
    "\n",
    "    def train_network(self):\n",
    "        neg_grad = self.neg_grad()\n",
    "        norm_factor = self.eta * np.linalg.norm(self.params) / np.linalg.norm(neg_grad)\n",
    "        self.params += norm_factor * neg_grad\n",
    "\n",
    "    def compute_accuracy(self, x_test: np.ndarray, y_test: np.ndarray):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.19245009, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.19245009],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.37796447, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.40824829, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizovat input!!!\n",
    "\n",
    "x = np.array(data[CZ_ALPHABET])\n",
    "y = np.array(data['expected'])\n",
    "\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST= train_test_split(x, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "# normalize input?\n",
    "Ns = len(X_TRAIN)\n",
    "Ni = len(CZ_ALPHABET)\n",
    "No = len(output_dict)\n",
    "ALPHA = 5\n",
    "FUNC = 'sigmoid'\n",
    "ETA = 0.05\n",
    "N_ITER = 8\n",
    "\n",
    "X_TRAIN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural network with a sample of 5007 names.\n",
      "Initialising a neural network with shape (43, 20, 14)\n",
      "Activation function is set to sigmoid\n",
      "Learning rate is set to 0.05\n",
      "Network size: [43 20 14]\n",
      "[array([0.12019656, 0.2961402 , 0.11872772, 0.31798318, 0.41426299,\n",
      "       0.0641475 , 0.69247212, 0.56660145, 0.26538949, 0.52324805,\n",
      "       0.09394051, 0.5759465 , 0.9292962 , 0.31856895, 0.66741038,\n",
      "       0.13179786, 0.7163272 , 0.28940609, 0.18319136, 0.58651293]), array([0.02010755, 0.82894003, 0.00469548, 0.67781654, 0.27000797,\n",
      "       0.73519402, 0.96218855, 0.24875314, 0.57615733, 0.59204193,\n",
      "       0.57225191, 0.22308163, 0.95274901, 0.44712538])]\n",
      "Starting learning session\n",
      "Cost before 1.0039495219832373\n",
      "Currently at i = 0\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [42], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m s \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCurrently at i = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 27\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m e \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIteration took \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me \u001B[38;5;241m-\u001B[39m s\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn [40], line 124\u001B[0m, in \u001B[0;36mNN.train_network\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_network\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 124\u001B[0m     neg_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mneg_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m     norm_factor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meta \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams) \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(neg_grad)\n\u001B[0;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m norm_factor \u001B[38;5;241m*\u001B[39m neg_grad\n",
      "Cell \u001B[1;32mIn [40], line 120\u001B[0m, in \u001B[0;36mNN.neg_grad\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;66;03m# numerical gradient - SLOW\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m     grad \u001B[38;5;241m=\u001B[39m \u001B[43mnd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGradient\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcost_of_sample\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39mgrad\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\core.py:490\u001B[0m, in \u001B[0;36mGradient.__call__\u001B[1;34m(self, x, *args, **kwds)\u001B[0m\n\u001B[0;32m    489\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m--> 490\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(Gradient, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(np\u001B[38;5;241m.\u001B[39matleast_1d(x)\u001B[38;5;241m.\u001B[39mravel(), \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfull_output:\n\u001B[0;32m    492\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(), result[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\core.py:431\u001B[0m, in \u001B[0;36mJacobian.__call__\u001B[1;34m(self, x, *args, **kwds)\u001B[0m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m--> 431\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(Jacobian, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(np\u001B[38;5;241m.\u001B[39matleast_1d(x), \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\core.py:288\u001B[0m, in \u001B[0;36mDerivative.__call__\u001B[1;34m(self, x, *args, **kwds)\u001B[0m\n\u001B[0;32m    286\u001B[0m x_i \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x)\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39merrstate(divide\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m, invalid\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 288\u001B[0m     results, f_xi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_derivative\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m     derivative, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extrapolate(\u001B[38;5;241m*\u001B[39mresults)\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfull_output:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\core.py:423\u001B[0m, in \u001B[0;36mJacobian._derivative_nonzero_order\u001B[1;34m(self, x_i, args, kwds)\u001B[0m\n\u001B[0;32m    421\u001B[0m steps, step_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_steps(x_i)\n\u001B[0;32m    422\u001B[0m fxi \u001B[38;5;241m=\u001B[39m f(x_i)\n\u001B[1;32m--> 423\u001B[0m results \u001B[38;5;241m=\u001B[39m [diff(f, fxi, x_i, h) \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m steps]\n\u001B[0;32m    425\u001B[0m steps2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_steps(steps, x_i, fxi)\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_richardson_rule(step_ratio, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrichardson_terms)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\core.py:423\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    421\u001B[0m steps, step_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_steps(x_i)\n\u001B[0;32m    422\u001B[0m fxi \u001B[38;5;241m=\u001B[39m f(x_i)\n\u001B[1;32m--> 423\u001B[0m results \u001B[38;5;241m=\u001B[39m [\u001B[43mdiff\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfxi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m steps]\n\u001B[0;32m    425\u001B[0m steps2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_steps(steps, x_i, fxi)\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_richardson_rule(step_ratio, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrichardson_terms)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\finite_difference.py:122\u001B[0m, in \u001B[0;36mJacobianDifferenceFunctions._central\u001B[1;34m(f, f_x, x, h)\u001B[0m\n\u001B[0;32m    120\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(x)\n\u001B[0;32m    121\u001B[0m steps \u001B[38;5;241m=\u001B[39m JacobianDifferenceFunctions\u001B[38;5;241m.\u001B[39mincrements(n, h)\n\u001B[1;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([(f(x \u001B[38;5;241m+\u001B[39m hi) \u001B[38;5;241m-\u001B[39m f(x \u001B[38;5;241m-\u001B[39m hi)) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hi \u001B[38;5;129;01min\u001B[39;00m steps])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\finite_difference.py:122\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    120\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(x)\n\u001B[0;32m    121\u001B[0m steps \u001B[38;5;241m=\u001B[39m JacobianDifferenceFunctions\u001B[38;5;241m.\u001B[39mincrements(n, h)\n\u001B[1;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([(f(x \u001B[38;5;241m+\u001B[39m hi) \u001B[38;5;241m-\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mhi\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2.0\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hi \u001B[38;5;129;01min\u001B[39;00m steps])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numdifftools\\core.py:257\u001B[0m, in \u001B[0;36mDerivative._get_functions.<locals>.export_fun\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexport_fun\u001B[39m(x):\n\u001B[1;32m--> 257\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fun(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "Cell \u001B[1;32mIn [40], line 112\u001B[0m, in \u001B[0;36mNN.cost_of_sample\u001B[1;34m(self, params)\u001B[0m\n\u001B[0;32m    110\u001B[0m sample_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain)\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams \u001B[38;5;241m=\u001B[39m params\n\u001B[1;32m--> 112\u001B[0m costs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcost(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain[i], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpected[i]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(sample_size)])\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(costs)\n",
      "Cell \u001B[1;32mIn [40], line 112\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    110\u001B[0m sample_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain)\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams \u001B[38;5;241m=\u001B[39m params\n\u001B[1;32m--> 112\u001B[0m costs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcost\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpected\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(sample_size)])\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(costs)\n",
      "Cell \u001B[1;32mIn [40], line 105\u001B[0m, in \u001B[0;36mNN.cost\u001B[1;34m(self, vector, expected)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcost\u001B[39m(\u001B[38;5;28mself\u001B[39m, vector: np\u001B[38;5;241m.\u001B[39marray, expected: np\u001B[38;5;241m.\u001B[39marray):\n\u001B[1;32m--> 105\u001B[0m     prediction \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvector\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m     c \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum((prediction \u001B[38;5;241m-\u001B[39m expected) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m c\n",
      "Cell \u001B[1;32mIn [40], line 96\u001B[0m, in \u001B[0;36mNN.predict\u001B[1;34m(self, vector)\u001B[0m\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeed_input(vector)\n\u001B[1;32m---> 96\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpropagate_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_output()\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prediction\n",
      "Cell \u001B[1;32mIn [40], line 79\u001B[0m, in \u001B[0;36mNN.propagate_forward\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_func(np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW[i], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i]) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mB[i])\n\u001B[0;32m     78\u001B[0m \u001B[38;5;66;03m# last layer using softmax\u001B[39;00m\n\u001B[1;32m---> 79\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m softmax(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mB[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "hidden_layer_size = int(np.ceil(calculate_hidden_layer_size(Ns=Ns, Ni=Ni, No=No, alpha=ALPHA)))\n",
    "network_size = np.array([Ni, hidden_layer_size, No], dtype=object)\n",
    "\n",
    "\n",
    "print(f'Training the neural network with a sample of {Ns} names.')\n",
    "print(f'alpha = {ALPHA} → optimal hidden layer size: {hidden_layer_size}')\n",
    "print(f'Initialising a neural network with shape {(Ni, hidden_layer_size, No)}')\n",
    "print(f'Activation function is set to {FUNC}')\n",
    "print(f'Learning rate is set to {ETA}')\n",
    "\n",
    "\n",
    "network = NN(size=network_size, func=FUNC, x_train=X_TRAIN[:700], y_train=Y_TRAIN[:700], eta=ETA, backpropagation=False)\n",
    "\n",
    "print(network.W[0].shape)\n",
    "start = time.time()\n",
    "print('Starting learning session')\n",
    "# network.cost_of_sample(vector_sample=vector_sample, expected_sample=expected_sample)\n",
    "print('Cost before', network.cost_of_sample(network.params))\n",
    "\n",
    "for i in range(N_ITER):\n",
    "    s = time.time()\n",
    "    print(f'Currently at i = {i}', end='\\r')\n",
    "    network.train_network()\n",
    "    e = time.time()\n",
    "    print(f'Iteration took {e - s:.2f} seconds')\n",
    "\n",
    "print('Cost after', network.cost_of_sample(network.params))\n",
    "end = time.time()\n",
    "print(f'Finished in {end-start:.2f} seconds')\n",
    "\n",
    "# print(network.cost_of_sample(vector_sample=vector_sample, expected_sample=expected_sample))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "0.034350535229269855"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['prediction'] = data['Obec'].apply(lambda x: output_dict[np.argmax(network.predict_word(word=x))])\n",
    "data['is_correct'] = data['Kraj'] == data['prediction']\n",
    "\n",
    "data['is_correct'].mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.923986673355103\n"
     ]
    }
   ],
   "source": [
    "def function(u: np.array, v:np.array):\n",
    "    return np.cross(u, v)\n",
    "\n",
    "function(np.array([1, 0]), np.array([0, 1]))\n",
    "\n",
    "u = np.array([1, 0, 0])\n",
    "v = np.array([0, 1, 0])\n",
    "f(u, v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1., 1.])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(u, v):\n",
    "    return u + v\n",
    "\n",
    "u = np.array([0, 0, 0])\n",
    "v = np.array([1, 2, 3])\n",
    "\n",
    "np.diagonal(nd.Gradient(f)(u, v))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 3)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.tile([1, 2, 3], (3, 1))\n",
    "arr.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ";"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
